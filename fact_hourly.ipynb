{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"ac08ac6c-93d4-4e31-9b25-f30091b6062a","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","from datetime import datetime\n","import uuid\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"f75dd72e-4838-4d32-91aa-1454fa7c0347","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","session = SparkSession.builder.appName('sparksession').getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"42f37a4b-3dd4-4bf7-b543-20aad4bd52cf","showTitle":false,"title":""}},"outputs":[],"source":["%run /Users/user/actionLogger"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"2c08636b-3f41-469a-9ef6-6b3bbc94a479","showTitle":false,"title":""}},"outputs":[],"source":["\n","log_schema = StructType([\n","    StructField(\"id\",StringType()),\n","    StructField(\"load_type\", StringType()),\n","    StructField(\"table_name\", StringType()),\n","    StructField(\"process_start_time\", TimestampType()),\n","    StructField(\"process_end_time\", TimestampType()),\n","    StructField(\"status\", StringType()),\n","    StructField(\"comments\", StringType()),\n","    StructField(\"start_date_time\", TimestampType()),\n","    StructField(\"end_date_time\", TimestampType()),\n","    StructField(\"created_on\", TimestampType()),\n","    StructField(\"created_by\", StringType())\n","\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"16b25595-c200-4ced-af9a-f22a0bc2f82b","showTitle":false,"title":""}},"outputs":[],"source":["\n","IDudf = udf(lambda: str(uuid.uuid4()), StringType())"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"63215003-9efb-40b9-a8f2-92a256d136fe","showTitle":false,"title":""}},"outputs":[],"source":["def lookup_data():\n","    \n","    data = session.read.format('delta').load('dbfs:/databases/weather/cleaned_weather_table')\n","    dim_location = session.read.format('delta').load('dbfs:/databases/weather/dim_location')\n","    dim_time = session.read.format('delta').load('dbfs:/databases/weather/dim_time')\n","    dim_date = session.read.format('delta').load('dbfs:/databases/weather/dim_date')\n","\n","\n","    dim_time = dim_time.withColumn('hr',hour(col('Hour')))\n","\n","    hr_data = data.withColumn('hr',hour(col('datetime')))\n","    hr_data = hr_data.withColumn('Date',to_date(col('datetime')))\n","\n","    hr_data = hr_data.join(dim_time,how='inner', on ='hr')\n","    hr_data = hr_data.join(dim_date, how = 'inner', on = 'Date')\n","\n","    hr_data = hr_data.withColumn(\"Fact_HourID\", IDudf())\n","\n","    hr_data = hr_data.select(\n","                         col('Fact_HourID'),\n","                         col('TimeID').alias('HourID'),\n","                         col('Date_id').alias('DateID'),\n","                         col('city_id').alias('CityID'),\n","\n","                         col('temp').cast('double').alias('Temperature'),\n","                         col('pressure').cast('double').alias('Pressure'),\n","                         col('humidity').cast('double').alias('Humidity'),\n","                         col('clouds').alias('Clouds'),\n","                         col('visibility').cast('double').alias('Visibility'),\n","                         col('wind_speed').cast('double').alias('Wind_speed'),\n","                         col('wind_deg').cast('double').alias('Wind_degree'),\n","                         col('wind_gust').cast('double').alias('Wind_gust'),\n","                         col('created_on'),\n","                         \n","        \n","                            )\n","    return hr_data"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"7c00db1e-fda0-4fd6-baf1-57edeaa8ef43","showTitle":false,"title":""}},"outputs":[],"source":["# hr_data = lookup_data()\n","# fact_hourly = session.read.format('delta').load('dbfs:/databases/weather/fact_hourly')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b6fba810-30af-4aba-9950-62c7008a47d8","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import col, row_number\n","from pyspark.sql.window import Window\n","\n","def write_to_fact_hourly(hr_data):\n","    try:\n","        fact_hourly = session.read.format('delta').load('dbfs:/databases/weather/fact_hourly')   \n","         \n","        fct = fact_hourly.drop('Fact_HourID')\n","        fct = facct_hourly.drop('is_forecasted')\n","        hrly_data = hr_data.drop('Fact_HourID')\n","        \n","        comparison_columns = ['HourID','DateID','CityID']\n","\n","        new = fct.unionAll(hrly_data.subtract(fct))\n","\n","        window_spec = Window.partitionBy(comparison_columns)\\\n","                    .orderBy(col('created_on').desc())\n","        new = new.withColumn('row_number',row_number().over(window_spec))\n","\n","        new = new.filter(col('row_number') == 1)\n","        new = new.drop('row_number')\n","        new = new.withColumn(\"Fact_HourID\", IDudf())\n","        new = new.withColumn(\"is_forecasted\", lit(\"N\"))\n","        \n","        new.write.option(\"overwriteSchema\", \"true\")\\\n","            .format('delta')\\\n","            .mode('overwrite')\\\n","            .save('dbfs:/databases/weather/fact_hourly')   \n","    except: \n","        hr_data.write.option(\"overwriteSchema\", \"true\")\\\n","            .format('delta')\\\n","            .mode('overwrite')\\\n","            .save('dbfs:/databases/weather/fact_hourly')   \n","       "]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"06a51299-1318-419e-8a29-80ca246e24df","showTitle":false,"title":""}},"outputs":[],"source":["def load_fact_hourly():\n","    \n","    process_start_time =  session.sql(\"SELECT current_timestamp()\").collect()[0][0]\n","    log_dict = {'id': str(uuid.uuid4().hex),\n","            'load_type': 'fact_load_hourly',\n","            'table_name': 'fact_hourly',\n","            'process_start_time':  session.sql(\"SELECT current_timestamp()\").collect()[0][0],\n","            'process_end_time': session.sql(\"SELECT current_timestamp()\").collect()[0][0],\n","            'start_date_time':  session.sql(\"SELECT current_timestamp()\").collect()[0][0],\n","            'end_date_time':  session.sql(\"SELECT current_timestamp()\").collect()[0][0]}\n","    \n","    log = action_logger(log_dict)\n","        \n","    \n","    try:\n","        hr_data = lookup_data()\n","        data = session.read.format('delta').load('dbfs:/databases/weather/cleaned_weather_table')\n","        \n","        log_dict = {\n","               'process_start_time' : process_start_time,\n","               'process_end_time' : session.sql(\"SELECT current_timestamp()\").collect()[0][0],\n","               'status' : 'extracting',\n","               'start_date_time' : data.select(min('created_on')).first()[0],\n","               'end_date_time' : data.select(max('created_on')).first()[0]}\n","            \n","        log.action(log_dict)\n","        \n","        \n","        write_to_fact_hourly(hr_data)\n","        \n","        \n","            \n","        log_dict = {\n","               'process_start_time' : process_start_time,\n","               'process_end_time' : session.sql(\"SELECT current_timestamp()\").collect()[0][0],\n","               'status' : 'completed',\n","               'start_date_time' : data.select(min('created_on')).first()[0],\n","               'end_date_time' : data.select(max('created_on')).first()[0]}\n","            \n","        log.action(log_dict)    \n","            \n","            \n","    except Exception as e:\n","        error_dict = {\n","               'process_start_time' : process_start_time,\n","               'process_end_time' : session.sql(\"SELECT current_timestamp()\").collect()[0][0],\n","               'status' : 'error',\n","               'error_data' : e,\n","               'start_date_time' : session.sql(\"SELECT current_timestamp()\").collect()[0][0],\n","               'end_date_time' : session.sql(\"SELECT current_timestamp()\").collect()[0][0]\n","        }\n","        log.action(error_dict)\n","            "]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"fact_hourly","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
